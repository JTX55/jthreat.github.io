<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>About | The Ethics of information technoology</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="About" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This website is about discussing the many aspects and implications of ethics in the information technology field. We talk about everything from the implications of our finicial systems running on blockchain to the econmoics of advanced A.I. like CHAT-GPT(LLM) and beyond. If that sounds compelling to you please come join us." />
<meta property="og:description" content="This website is about discussing the many aspects and implications of ethics in the information technology field. We talk about everything from the implications of our finicial systems running on blockchain to the econmoics of advanced A.I. like CHAT-GPT(LLM) and beyond. If that sounds compelling to you please come join us." />
<link rel="canonical" href="http://localhost:4000/about/" />
<meta property="og:url" content="http://localhost:4000/about/" />
<meta property="og:site_name" content="The Ethics of information technoology" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="About" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"This website is about discussing the many aspects and implications of ethics in the information technology field. We talk about everything from the implications of our finicial systems running on blockchain to the econmoics of advanced A.I. like CHAT-GPT(LLM) and beyond. If that sounds compelling to you please come join us.","headline":"About","name":"The Ethics of information technoology","url":"http://localhost:4000/about/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="The Ethics of information technoology" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">The Ethics of information technoology</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
        <div class="trigger"><a class="page-link" href="/discussion/">Discussion</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Discussion</h1>
  </header>

  <div class="post-content">
    <p>This website is about discussing the many aspects and implications of ethics in the information technology field. We talk about everything from the implications of our finicial systems running on blockchain to the econmoics of advanced A.I. like CHAT-GPT(LLM) and beyond. If that sounds compelling to you please come join us.
<p>You can find the source code for Minima at GitHub:
<a href="https://github.com/jekyll">jekyll</a> /
<a href="https://github.com/jekyll/minima">minima</a></p>

<p>You can find the source code for Jekyll at GitHub:
<a href="https://github.com/jekyll">jekyll</a> /
<a href="https://github.com/jekyll/jekyll">jekyll</a></p>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Ethics of information technoology</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Ethics of information technoology</li><li><a class="u-email" href="mailto:jthreat@gmu.edu">jthreat@gmu.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Article Translation Draft
            Name: Joshua Threatt
            Part One
            Today we review the foundation large language model technology in an academic paper aply 
            named, “Attention Is All You Need”. You may better know it as the technology behind CHAT-4,
            PaLM, Bard, LLamma, Bloom, Falcon 9 and other such related technologies and other such 
            related technologies. These technologies fall into a broad class of network architecture called 
            neural networks; which then fall into a subclass called large language models. Large Language 
            models can be seen as a kind of AI system with the ability to obtain embodies knowledge about 
            language syntax and ontology. It does this through a novel computing system called the 
            transformer. We will discuss this, the attention mechanism and everything else you need to know
            in the reading ahead.
            Abstract
            From the very beginning of the abstract, we can see the authors departer from the older more 
            ubiquitous architecture such as recurrent or convolutional neural network models. We then that 
            the best models at the time used attention mechanisms; the authors then propose a simplified 
            architecture with only attention mechanisms called the transformer. They run experiments which 
            confirm the enhanced performance of their mode; this includes better quality and a system that 
            can run operations concurrently. Signs of its early success are the models achieving a new high 
            BLEU score in the French to English translation. A model's BLEU score is a measure of the 
            quality of its translation from one natural language to another. 
            Attention & Transformers
            An essential aspect of the of this technology is the attention mechanism. In neural networks 
            attention mimics the attention we see in biological neurological systems. In effect meaning more 
            importance and weight is allocated to certain portions of an image, text, etc that others. These 
            weights can be thought of as many tiny pieces of a pie or small parts that add up to a whole. This
            is where the transformer comes in; it is a simplified architecture due to it scrapping the more 
            complex recurrent/convolutional and relying entirely on stacked self-attention units. The 
            simplified architecture also allows the system to move from serial processing of information to 
            concurrent. This is like going from a one way one lane road to an 8-lane superhighway. Each 
            lane in this super highway being a head in the multi-head attention mechanism. 
            Encoder & Decoder
            In the transformer architecture the encoding and decoding mechanisms are essential. On the 
            encoding side large language models use a method called byte pair encoding. This takes the input
            and transforms it by putting it through the series of weights that we saw earlier. The decoding 
            mechanism is very similar to the encoding except that it uses the output of the encoding as its 
            input then gives the final output. Both the encoder and decoder have 6 identical layers with each 
            having 2 sub layers. The first of these layers is a multi-head self-attention mechanism. With 
            multi-head self-attention rather than performing a single attention function on the input (queries, 
            keys, values) it performs multiple attention functions in parallel. The output of these functions 
            are then concatenated or combined to give a final value. The great magic of this whole process is
            that it allows the model to represent different subspaces at different positions in parallel. Before 
            moving on we need to discuss one more important feature of the decoding-self-attention layer. 
            Information flow can only go in direction to retain auto-regression in the system. Auto-
            regression is a process by which the system predicts future outputs with its previous outputs. The
            predictions are self-correcting as the outputs continually optimize toward a certain, a kind of 
            unsupervised learning. 
            Why Self-Attention
            On a base level what is the reason for the effectiveness of self-attention is a numbers game. No, 
            we aren't getting deep into math, but you will understand the difference in ‘scale’. When we 
            compare the different layer types there are three important variables to keep in mind. These 
            include the total complexity in a layer; amount of computation parallelizable and the length of 
            the paths between long range dependencies. The shorter the path between two dependencies the 
            easier it is to learn long range paths. As you can see on the chart below under the ‘Maximum 
            Path Length’ the only path with a constant number of sequential number of operations is the self-
            attention layer. Also take note of the ‘Complexity per layer’ column; the only layer that has two 
            variables and no square for the d(dimension) variable is the self-attention layer. All this adds up 
            to mean that under the majority of circumstances the self-attention layer is faster and more 
            efficient than recurrent or convolutional networks. This efficiency and increased parallelism are 
            creating a large difference in scale in what these different systems can handle. 
            Part two
            Analogies
            There are a few important aspects I considered when writing this paper. One of these is the use 
            of analogies. This paper was made for a general audience and many of the of the concepts being 
            discussed are abstract. Since we are discussing several abstract topics being weaved together, I 
            found it fit to make the conversion more concrete at time. As far as analogies go it's important to 
            reference something that is common knowledge and easy to picture in one's own head. For 
            example, when discussing the weights in a nueral network I referenced small pieces of a pie 
            adding up to a whole pie.
            Points of Emphasis 
            I’ve had a consistent problem with writing on certain topics in this class and this draft is the 
            worst example. The paper I’m writing about could be described far more thoroughly and in much
            greater detail. However, I am constrained by the word limit and must therefore choose the most 
            important aspects to concentrate on. Though it wasn't easy it was good practice differentiating 
            what's important to know for a general understanding of the topic. When writing this it was very 
            important not to get stuck in the weeds. There were lots of equations and jargon that might be 
            necessary for a deep dive but not for a general understanding. 
            
            <p>
            Work cited:     Vaswani Ashish, Shazeer Noam, Parmer Niki, Uszkoreit Jakob, Jones Llion, Gomez A Aiden, 
            Kaiser Lukasz, Polosukin Lllia. “Attention is all you need”. arXiv. 2017
           </p>
            </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
